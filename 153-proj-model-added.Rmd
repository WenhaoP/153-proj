---
title: "Time series analysis on the stock price of Tesla Inc."
author: "Wenhao Pan (3034946058), Ruojia Zhang, Mengzhu Sun, Xiangxi Wang, Mingmao Sun"
date: "November 13, 2021"
output:
  pdf_document: 
    toc: true
    number_sections: true
urlcolor: blue
---
\newpage
```{r include = FALSE}
# Template source: https://github.com/alexpghayes/rmarkdown_homework_template
knitr::opts_chunk$set(
  echo = FALSE, # don't show code
  warning = FALSE, # don't show warnings
  message = FALSE, # don't show messages (less serious warnings)
  cache = FALSE, # set to TRUE to save results from last compilation
  fig.align = "center", # center figures
  fig.height = 4
)
library(ggplot2)
library(patchwork)
library(astsa)
library(TSA)
library(forecast)
set.seed(153) # make random results reproducible
```

# Abstract

# Introduction

# Data Description

```{r}
# load data
stock = read.csv("../Desktop/TSLA.csv")
stock$Date = as.Date(stock$Date)
# Extract last 300 days
n = 300
t = 1:n
stock_300 = tail(stock, n)
stock_300 = stock_300[c('Date','Close')]
stock_300$t = t
```

# Exploratory Data Analysis

To obtain a comprehensive understanding of the data, we conduct explanatory data analysis (EDA) first. Figure 1(a) is the time series plot of all the given time points. We observe that the stock prices of Tesla before 2020 are averagely and considerably lower than those after 2020. The significantly different scales of different parts of the time series make it hard to visually examine the trend and seasonality pattern of the time series. Moreover, since we are majorly interested in the recent activities of Tesla, we do not have to analyze all the available data. Therefore, for the sake of interest and convenience, we decide only to analyze the last 300 time points, which cover the period from `2020-08-26` to `2021-11-02` excluding weekends. Thus, whenever we use the word "data" in the following analysis, we implicitly mean the time series of the last three hundred time points. 

```{r, fig.cap="(a) Time series plot of all available trading days. (b) Time series plot of last 300 trading days", fig.height=6, out.width="80%", fig.height=4.5}
# Line plot of all the data
full_data <- ggplot(data = stock, aes(x = Date, y = Close)) +
  geom_line() +
  ylab("Close Price (Dollars)") +
  ggtitle("(a)")
# Line plot of last 300 data
last_300 <- ggplot(data = stock_300, aes(x = Date, y = Close)) +
  geom_line() +
  ylab("Close Price (Dollars)") +
  ggtitle("(b)")
full_data / last_300
```

Figure 1(b) is the time series plot of the close prices of Tesla in the last three hundred trading days before and including `2021-11-02`. We first observe that our data is roughly homoscedastic based on Figure 1(b). To verify our observation, we try the square root and natural log transformations and see whether they effectively stabilize the variance of the time series. Their plots are below in Figure 2.

```{r, fig.cap="(a): Original time series. (b): Square root transfromed time seires. (c): Natural log transformed time series.", fig.height=3}
# Line plot of last 300 data
last_300 <- ggplot(data = stock_300, aes(x = Date, y = Close)) +
  geom_line() +
  ylab("Close Price (Dollars)") +
  ggtitle("(a)") + 
  theme(text = element_text(size = 8)) + 
  guides(x =  guide_axis(angle = 90))
# Line plot of last 300 data with square root transformation
last_300_sqrt <- ggplot(data = stock_300, aes(x = Date, y = Close)) +
  geom_line() +
  scale_y_sqrt() +
  ylab("Close Price (Dollars)") +
  ggtitle("(b)") + 
  theme(text = element_text(size = 8)) +
  guides(x =  guide_axis(angle = 90))
# Line plot of last 300 data with natural log transformation
last_300_log <- ggplot(data = stock_300, aes(x = Date, y = Close)) +
  geom_line() +
  scale_y_log10() +
  ylab("Close Price (Dollars)") +
  ggtitle("(c)") + 
  theme(text = element_text(size = 8)) +
  guides(x =  guide_axis(angle = 90))
last_300 + last_300_sqrt + last_300_log
```
We can see that both transformations unnecessarily increase the variance of the time series before mid-November in 2020 and do not change the variance of other time series data. Although both transformations shorten the vertical distance between the maximum and minimum of the time series after Oct. 2021, the spike after Oct. 2021 is more like an increasing trend than a considerable fluctuation. In short, both transformations are redundant, and we do not need to use any variance stabilizing transformation. 

Back to Figure 1(b), intuitively, the data is not stationary because of a nonlinear and generally increasing trend. The trend first increases until around Feb. 2021 and then decreases until around Mid-May. 2021. Finally, the trend increases again until the end of the time series. Nonetheless, we do not observe an obvious or significant seasonality pattern. It matches the intuition since the granularity of our data is day, and the structure of stock price data is too complicated to have a seasonality pattern.

In conclusion, based on all the previous discussions in EDA, we decide to construct possible models on the original time series data, including only the last three hundred time points. 

# Model Construction

With a comprehensive understanding of our data, we start to experiment and construct different time series model. We choose and build two non-parametric signal models of the trend and seasonality in our data. We aim to make the residuals approximately weekly stationary. We do not consider any parametric trend model because we think the trend of the stock price data is too complicated to be modeled by a parametric model, such as a high-order polynomial. Certainly, we could use a 15 or 20 order polynomial, but it may overfit the training data and produce imprecise predictions. We do not consider a parametric seasonality model either because we do not find a clear seasonality pattern in our data by the EDA. Finally, based on each signal model, we provide two ARMA models or its extension, such as SARMA or ARIMA, to whiten the residuals of the signal model. Thus, we have four candidate models, and we will explain how we select a final model among them in the next section.

## Non-parametric Signal Model: exponential smoothing

In this signal model, we choose exponential smoothing with weight $\alpha = 0.8$ and lag $k = 10$ and a seasonal differencing with period $d = 5$. 

```{r, fig.cap="(a): Time series plot of the original data and fitted values. (b): The residual plot of exponential smoothing.", fig.height=3}
par(mfrow = c(1, 2), cex = 0.65, lwd = 0.8)
# exponential filter 
alpha = 0.8
lag = 10
filter_weights = alpha^(1:lag)
filter_weights = filter_weights/sum(filter_weights)
filtered_stock = filter(stock_300$Close, filter_weights, sides = 1)
# Plot the original data and fitted values
plot(stock_300$t, stock_300$Close, type = 'l', main = "(a)", xlab = "Time", ylab = "Close Price")
lines(stock_300$t, filtered_stock, col = 'red')
filtered_stock = na.omit(filtered_stock)
log_stock = stock_300$Close[-1:-(lag - 1)]
res = log_stock - filtered_stock
plot(res, main = "(b)", xlab = "Time", ylab = "Residuals")
```

We experiment with different combinations of $\alpha$ and $k$ with a careful consideration of overfitting issue. we choose $k=10$ as the final value because we want to only use past two weeks, which are ten days in our data, to forecast. We choose $\alpha = 0.8$ as the final value because we think it best balances the smoothing effect and the capture of trend pattern among $(0,1)$. Indeed, the smoothing line in Figure 4(a) fits the data in the way that we want. Note that we lose the first nine time points due to the computation process of the exponential smoothing.

However, the residual plot Figure 4(b) is fairly non-stationary, as it has cycling fluctuation pattern and still slightly nonlinear trend. It might be due to that we intentionally let exponential smoothing not fit the data perfectly. Next, We use the seasonal differencing with period $d = 5$, which is one week in our data, to further make the residuals more stationary.

```{r, results='hide', fig.cap="(a): Time series plot of the seasonal differenced (d = 5) residuals from the previous smoothing.", out.width="80%"}
# seasonal differencing
diff_res = diff(res, lag = 5)
plot(diff_res, main = "(a)", ylab = "Differenced Data")
```

Indeed, now the differenced residuals become more stationary. There seems to be a contradiction that recalling in EDA, we claim that there is not a clear seasonality in our data. However, the effect of the seasonal differencing here implies a possible seasonality with period $d=5$. We think it is because the seasonal differencing is actually removing the remaining trend left by the exponential smoothing instead of the seasonality. Nevertheless, We believe that the time series of the differenced residuals shown in Figure 5(a) is stationary enough for us to build ARMA models on it.

## Non-parametric Signal Model: second-order differencing 

In this model, we choose the second-order differencing to remove the trend. We observe that after the first-order differencing, there is still some trend pattern, such as the increasing one between $270$ and $300$, as shown by Figure TODO. This matches our previous analysis that the trend of our data is nonlinear in EDA. Thus, we take another differencing and acquire the second-order differencing data shown in Figure TODO.

```{r, fig.cap="(a): The first-order differenced data. (b): The second-order differenced data.", fig.height=3}
par(mfrow = c(1, 2), cex = 0.65, lwd = 0.5)
# first order differencing
stock_d = diff(stock_300$Close)
# second order differencing
stock_dd = diff(stock_d)
plot(stock_d, type = 'l', main = "(a)", ylab = "Differenced Data")
plot(stock_dd, type = 'l', main = "(b)", ylab = "Differenced Data")
```

The second-order differenced time series is more stationary than the first-order differenced time series. We can keep trying more higher-order differencings, but they may overfit our data. Therefore, we think the second-order differenced time series is already stationary enough for us to build ARMA model on it.

# Model Comparision and Selection

## [Exponential Smoothing + Seasonal Differencing] Model 2 (Ruojia Zhang)

Here we fit seasonal ARMA model of the seasonal differenced residuals after reducing the trend using exponential smoothing.

```{r}
acf2(diff_res)
```

From the acf and pacf plots of the differenced residuals above we can observe negative spike at lag = 5 for both plots. Neither the acf or the pacf plot shows reasonable cutoff. So here we need to choose a model where p > 0 and q > 0.

First, using auto.arima() to see the model suggested by R:

```{r}
auto.arima(diff_res)
```

```{r}
trial <- sarima(diff_res, p = 2, d = 0, q = 2, P = 0, D = 0, Q = 0, S = 0)
```
```{r}
trial$AIC
```

```{r}
trial$AICc
```

```{r}
trial$BIC
```

From the performance of the model recommended by R presented above, we can see that the model does not pass the Ljung-Box test (all the p-values are below the blue band). Combined with my observations from the acf and pacf plots, I adjusted the model parameters and tried the lower the AIC, AICc, and BIC values of the model at the same time. 

Models considered:

```{r}
attempt1 <- sarima(diff_res, p = 2, d = 0, q = 2, P = 0, D = 0, Q = 0, S = 5)
# does not pass the Ljung-box test
```
```{r}
attempt2 <- sarima(diff_res, p = 2, d = 0, q = 2, P = 1, D = 0, Q = 0, S = 5)
# does not pass the Ljung-box test
```

```{r}
attempt3 <- sarima(diff_res, p = 2, d = 0, q = 2, P = 0, D = 0, Q = 1, S = 5)
```
```{r}
print(attempt3$AIC)
print(attempt3$AICc)
print(attempt3$BIC)
# we want to lower the AIC, AICc, BIC values by keep adjusting the parameters
```


```{r}
attempt4 <- sarima(diff_res, p = 1, d = 0, q = 2, P = 0, D = 0, Q = 1, S = 5)
```
```{r}
print(attempt4$AIC)
print(attempt4$AICc)
print(attempt4$BIC)
# we want to lower the AIC, AICc, BIC values by keep adjusting the parameters
```

```{r}
attempt5 <- sarima(diff_res, p = 1, d = 0, q = 1, P = 0, D = 0, Q = 1, S = 5)
```
```{r}
print(attempt5$AIC)
print(attempt5$AICc)
print(attempt5$BIC)
# we want to lower the AIC, AICc, BIC values by keep adjusting the parameters
```

```{r}
attempt6 <- sarima(diff_res, p = 0, d = 0, q = 0, P = 0, D = 0, Q = 1, S = 5)
```
From these previous attempts we can see that attempt 5 generates the lowest AIC, AICc, and BIC values, and passes the Ljung-Box test.

In the end, keeping other parameters unchanged, I reduced the values for p and q from 2 to 1, and chose S = 5 and Q = 1, and the model presented p-values lying far beyond the blue band in the Ljung-Box statistic plot with relatively lower AIC, AICc, and BIC values than before:

```{r}
model2 <- sarima(diff_res, p = 1, d = 0, q = 1, P = 0, D = 0, Q = 1, S = 5)
```

From the acf plot of the residuals, we can see that most of the sample residuals autocorrelations fit nicely inside the confidence band. From the normal q-q plot of the stanard residuals we can see that normal distribution is a convincing assumption. We also have large p-values that suggests weak groupwise residual autocorrelations in the plot for Ljung-Box statistic. The model seems like a good fit.

Then we report the AIC, AICc, BIC values for the model:

```{r}
model2$AIC
```

```{r}
model2$AICc
```

```{r}
model2$BIC
```

We then perform time-series cross validation. For this model, we generate forecasts for every 10 days (2 weeks) and compute the sum of squares of errors of the forecasts. Then average the sum of squares of errors of forecasts over the days considered, which generates the mse of this model.

```{r}
# cross-validation
start <- 200
total_error <- 0

for (i in 0:9) {
  # train-val set split
  train <- window(stock_300$Close, end = start + i * 10 - 0.01)
  val <- window(stock_300$Close, start = start + i * 10, end = start + (i + 1) * 10- 0.01)
  
  # get stationary residuals
  filtered_train = filter(train, filter_weights, sides = 1)
  filtered_train = na.omit(filtered_train)
  cut_train <- train[-1:-(lag - 1)]
  exp_res = cut_train - filtered_train
  
  # exponential smoothing prediction
  exp_pred = numeric(10)
  values = tail(train, 10) # store the values for prediction
  for (i in 1:10){
    exp_pred[i] = sum(tail(values, 10) * rev(filter_weights)) 
    values <- append(values, exp_pred[i])
  }
  
  # ARMA prediction
  arma_pred <- sarima.for(exp_res, n.ahead = 10,  p = 1, d = 0, q = 1, P = 0, D = 0, Q = 1, S = 5)

  print(exp_pred)
  print(arma_pred$pred)
  m1_pred <- exp_pred + arma_pred$pred
  total_error <- total_error + mean((m1_pred - val)^2)
}

total_error / 10
```

```{r}
cv_model2 = total_error / 10
cat("cross validation error", cv_model2)
```

# Final Model

## Model interpretation

## Prediction

# Conclusion
