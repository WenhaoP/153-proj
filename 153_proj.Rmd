---
title: "Time series analysis on the stock price of Tesla Inc."
author: "Wenhao Pan (3034946058), Ruojia Zhang, Mengzhu Sun, Xiangxi Wang, Mingmao Sun"
date: "November 13, 2021"
output:
  pdf_document: 
    toc: true
    number_sections: true
urlcolor: blue
---
\newpage
```{r include = FALSE}
# Template source: https://github.com/alexpghayes/rmarkdown_homework_template
knitr::opts_chunk$set(
  echo = FALSE, # don't show code
  warning = FALSE, # don't show warnings
  message = FALSE, # don't show messages (less serious warnings)
  cache = FALSE, # set to TRUE to save results from last compilation
  fig.align = "center", # center figures
  fig.height = 4
)
library(ggplot2)
library(patchwork)
library(astsa)
library(TSA)
set.seed(153) # make random results reproducible
```

# Abstract

# Introduction

Due to the increasing focus on carbon neutrality, the industry of replacing non-sustainable energy with sustainable energy has boomed in the past few years. Electricity, as a relatively environment-friendly energy, has been considered as replacement of some traditional energy, such as gasoline and diesel.  Among all those enterprises pursing commercialized carbon neutrality, TSLA, as the largest electric car company, has been pioneering the fashion and aiming to transition the world to electric mobility. As the reflection of belief of the public, the stock price of TSLA has been sedentary for a period of time and has no evident increase until recent years. Therefore, we pick up the close price of TSLA stock of the recent 300 days to explore. In the following experiments, we utilize differencing, exponential smoothing, and fitting ARMA model, and combination of them to approximate the series.


```{r}
# load data
stock = read.csv("data/TSLA.csv")
stock$Date = as.Date(stock$Date)
Tsla = as.data.frame(stock["Close"])$Close
# Extract last 300 days
n = 300
t = 1:n
stock_300 = tail(stock, n)
stock_300 = stock_300[c('Date','Close')]
stock_300$t = t

tsla300 = as.data.frame(stock_300)$Close
```

# Data Description

The TSLA stock price comes from Yahoo Finance (https://finance.yahoo.com). The stock price dataset consists of open price, close price, high price, and low price. Since they has roughly similar trend, we choose close price to experiment on. The whole volume of data, which contains 2791 data points, has variance 39768.49, max price 1208.59, min price 4.01, mean price 112.4271. The recent-300-day data has variance 22697.1, max price 1208.59, min price 330.21, mean price 654.1912.

```{r}
variance  = c(var(Tsla),var(tsla300))
maxi = c(max(Tsla),max(tsla300))
mini = c(min(Tsla),min(tsla300))
meani = c(mean(Tsla),mean(tsla300))
len = c(length(Tsla),length(tsla300))
desc <- data.frame("Data" = c("whole volume Tsla data", "recent-300-day Tsla data"), "Variance" = variance, "Max" = maxi, "Min" = mini, "Mean" = meani, "Size" = len)
desc
```


# Exploratory Data Analysis

To obtain a comprehensive understanding of the data, we conduct explanatory data analysis (EDA) first. Figure 1(a) is the time series plot of all the given time points. We observe that the stock prices of Tesla before 2020 are averagely and considerably lower than those after 2020. The significantly different scales of different parts of the time series make it hard to visually examine the trend and seasonality pattern of the time series. Moreover, since we are majorly interested in the recent activities of Tesla, we do not have to analyze all the available data. Therefore, for the sake of interest and convenience, we decide only to analyze the last 300 time points, which cover the period from `2020-08-26` to `2021-11-02` excluding weekends. Thus, whenever we use the word "data" in the following analysis, we implicitly mean the time series of the last three hundred time points. 

```{r, fig.cap="(a) Time series plot of all available trading days. (b) Time series plot of last 300 trading days", fig.height=6, out.width="80%", fig.height=4.5}
# Line plot of all the data
full_data <- ggplot(data = stock, aes(x = Date, y = Close)) +
  geom_line() +
  ylab("Close Price (Dollars)") +
  ggtitle("(a)")

# Line plot of last 300 data
last_300 <- ggplot(data = stock_300, aes(x = Date, y = Close)) +
  geom_line() +
  ylab("Close Price (Dollars)") +
  ggtitle("(b)")

full_data / last_300
```

Figure 1(b) is the time series plot of the close prices of Tesla in the last three hundred trading days before and including `2021-11-02`. We first observe that our data is roughly homoscedastic based on Figure 1(b). To verify our observation, we try the square root and natural log transformations and see whether they effectively stabilize the variance of the time series. Their plots are below in Figure 2.

```{r, fig.cap="(a): Original time series. (b): Square root transfromed time seires. (c): Natural log transformed time series.", fig.height=3}
# Line plot of last 300 data
last_300 <- ggplot(data = stock_300, aes(x = Date, y = Close)) +
  geom_line() +
  ylab("Close Price (Dollars)") +
  ggtitle("(a)") + 
  theme(text = element_text(size = 8)) + 
  guides(x =  guide_axis(angle = 90))

# Line plot of last 300 data with square root transformation
last_300_sqrt <- ggplot(data = stock_300, aes(x = Date, y = Close)) +
  geom_line() +
  scale_y_sqrt() +
  ylab("Close Price (Dollars)") +
  ggtitle("(b)") + 
  theme(text = element_text(size = 8)) +
  guides(x =  guide_axis(angle = 90))

# Line plot of last 300 data with natural log transformation
last_300_log <- ggplot(data = stock_300, aes(x = Date, y = Close)) +
  geom_line() +
  scale_y_log10() +
  ylab("Close Price (Dollars)") +
  ggtitle("(c)") + 
  theme(text = element_text(size = 8)) +
  guides(x =  guide_axis(angle = 90))

last_300 + last_300_sqrt + last_300_log
```
We can see that both transformations unnecessarily increase the variance of the time series before mid-November in 2020 and do not change the variance of other time series data. Although both transformations shorten the vertical distance between the maximum and minimum of the time series after Oct. 2021, the spike after Oct. 2021 is more like an increasing trend than a considerable fluctuation. In short, both transformations are redundant, and we do not need to use any variance stabilizing transformation. 

Back to Figure 1(b), intuitively, the data is not stationary because of a nonlinear and generally increasing trend. The trend first increases until around Feb. 2021 and then decreases until around Mid-May. 2021. Finally, the trend increases again until the end of the time series. Nonetheless, we do not observe an obvious or significant seasonality pattern. It matches the intuition since the granularity of our data is day, and the structure of stock price data is too complicated to have a seasonality pattern.

In conclusion, based on all the previous discussions in EDA, we decide to construct possible models on the original time series data, including only the last three hundred time points. 

# Model Construction

With a comprehensive understanding of our data, we start to experiment and construct different time series model. We choose and build two non-parametric signal models of the trend and seasonality in our data. We aim to make the residuals approximately weekly stationary. We do not consider any parametric trend model because we think the trend of the stock price data is too complicated to be modeled by a parametric model, such as a high-order polynomial. Certainly, we could use a 15 or 20 order polynomial, but it may overfit the training data and produce imprecise predictions. We do not consider a parametric seasonality model either because we do not find a clear seasonality pattern in our data by the EDA. Finally, based on each signal model, we provide two ARMA models or its extension, such as SARMA or ARIMA, to whiten the residuals of the signal model. Thus, we have four candidate models, and we will explain how we select a final model among them in the next section.

## Non-parametric Signal Model: exponential smoothing

In this signal model, we choose exponential smoothing with weight $\alpha = 0.8$ and lag $k = 10$ and a seasonal differencing with period $d = 5$. 

```{r, fig.cap="(a): Time series plot of the original data and fitted values. (b): The residual plot of exponential smoothing.", fig.height=3}
par(mfrow = c(1, 2), cex = 0.65, lwd = 0.8)

# exponential filter 
alpha = 0.8
lag = 10
filter_weights = alpha^(1:lag)
filter_weights = filter_weights/sum(filter_weights)
filtered_stock = filter(stock_300$Close, filter_weights, sides = 1)

# Plot the original data and fitted values
plot(stock_300$t, stock_300$Close, type = 'l', main = "(a)", xlab = "Time", ylab = "Close Price")
lines(stock_300$t, filtered_stock, col = 'red')

filtered_stock = na.omit(filtered_stock)
log_stock = stock_300$Close[-1:-(lag - 1)]
res = log_stock - filtered_stock
plot(res, main = "(b)", xlab = "Time", ylab = "Residuals")
```

We experiment with different combinations of $\alpha$ and $k$ with a careful consideration of overfitting issue. we choose $k=10$ as the final value because we want to only use past two weeks, which are ten days in our data, to forecast. We choose $\alpha = 0.8$ as the final value because we think it best balances the smoothing effect and the capture of trend pattern among $(0,1)$. Indeed, the smoothing line in Figure 4(a) fits the data in the way that we want. Note that we lose the first nine time points due to the computation process of the exponential smoothing.

However, the residual plot Figure 4(b) is fairly non-stationary, as it has cycling fluctuation pattern and still slightly nonlinear trend. It might be due to that we intentionally let exponential smoothing not fit the data perfectly. Next, We use the seasonal differencing with period $d = 5$, which is one week in our data, to further make the residuals more stationary.

```{r, results='hide', fig.cap="(a): Time series plot of the seasonal differenced (d = 5) residuals from the previous smoothing.", out.width="80%"}
# seasonal differencing
diff_res = diff(res, lag = 5)
plot(diff_res, main = "(a)", ylab = "Differenced Data")
```

Indeed, now the differenced residuals become more stationary. There seems to be a contradiction that recalling in EDA, we claim that there is not a clear seasonality in our data. However, the effect of the seasonal differencing here implies a possible seasonality with period $d=5$. We think it is because the seasonal differencing is actually removing the remaining trend left by the exponential smoothing instead of the seasonality. Nevertheless, We believe that the time series of the differenced residuals shown in Figure 5(a) is stationary enough for us to build ARMA models on it.

## Non-parametric Signal Model: second-order differencing 

In this model, we choose the second-order differencing to remove the trend. We observe that after the first-order differencing, there is still some trend pattern, such as the increasing one between $270$ and $300$, as shown by Figure TODO. This matches our previous analysis that the trend of our data is nonlinear in EDA. Thus, we take another differencing and acquire the second-order differencing data shown in Figure TODO.

```{r, fig.cap="(a): The first-order differenced data. (b): The second-order differenced data.", fig.height=3}
par(mfrow = c(1, 2), cex = 0.65, lwd = 0.5)

# first order differencing
stock_d = diff(stock_300$Close)
# second order differencing
stock_dd = diff(stock_d)

plot(stock_d, type = 'l', main = "(a)", ylab = "Differenced Data")
plot(stock_dd, type = 'l', main = "(b)", ylab = "Differenced Data")
```
The second-order differenced time series is more stationary than the first-order differenced time series. We can keep trying more higher-order differencings, but they may overfit our data. Therefore, we think the second-order differenced time series is already stationary enough for us to build ARMA model on it.

```{r, echo = FALSE,fig.cap="(a): The first-order differenced data. (b): The second-order differenced data.", fig.height=3}
par(mfrow = c(1, 2), cex = 0.65, lwd = 0.5)
v = diff(diff(tsla300))
acf2((v))
```
On the PACF graph of second order differencing series v, the absolute value of PACF is decreasing significantly from lag 1 to lag 5. On the ACF graph of v, the evident cutoff of absolute value of ACF occurs at lag 1. Therefore, it is reasonable to approximate the series v with MA model. However, the ACF and PACF plot are not strictly fitted on the theoretical $MA(q),q \in \{4,5,6\}$ model, so conduct parameter tuning experiment on q and possibility of $AR(p),p \in \{0,1\}$.

```{r}
accuracy_1 = c()
accuracy_2 = c()
accuracy_3 = c()
accuracy_4 = c()
accuracy_5 = c()
accuracy_6 = c()
for(k in 0:12){
  Nt <- 10
  N <- length(v) - 10 - Nt * k
  x = c(1:N)
  new = data.frame(x = c((N + 1):(N + Nt)))
  train <- v[1:N]
  test <- v[(N+1):(N + Nt)]
  mod1_pred <- sarima.for(train, n.ahead = Nt, p = 0, d = 0, q = 4,plot=FALSE)
  mod2_pred <- sarima.for(train, n.ahead = Nt, p = 0, d = 0, q = 5,plot=FALSE)
  mod3_pred <- sarima.for(train, n.ahead = Nt, p = 0, d = 0, q = 6,plot=FALSE)
  mod4_pred <- sarima.for(train, n.ahead = Nt, p = 1, d = 0, q = 4,plot=FALSE)
  mod5_pred <- sarima.for(train, n.ahead = Nt, p = 1, d = 0, q = 5,plot=FALSE)
  mod6_pred <- sarima.for(train, n.ahead = Nt, p = 1, d = 0, q = 6,plot=FALSE)
  accuracy_1 = append(accuracy_1, (mean((test - as.vector(mod1_pred$pred))^2))^0.5)
  accuracy_2 = append(accuracy_2, (mean((test - as.vector(mod2_pred$pred))^2))^0.5)
  accuracy_3 = append(accuracy_3, (mean((test - as.vector(mod3_pred$pred))^2))^0.5)
  accuracy_4 = append(accuracy_4, (mean((test - as.vector(mod4_pred$pred))^2))^0.5)
  accuracy_5 = append(accuracy_5, (mean((test - as.vector(mod5_pred$pred))^2))^0.5)
  accuracy_6 = append(accuracy_6, (mean((test - as.vector(mod6_pred$pred))^2))^0.5)
}
cv1 = mean(accuracy_1)
cv2 = mean(accuracy_2)
cv3 = mean(accuracy_3)
cv4 = mean(accuracy_4)
cv5 = mean(accuracy_5)
cv6 = mean(accuracy_6)

model_1 = sarima(v, p = 0, d = 0, q = 4,details=FALSE)
model_2 = sarima(v, p = 0, d = 0, q = 5,details=FALSE)
model_3 = sarima(v, p = 0, d = 0, q = 6,details=FALSE)
model_4 = sarima(v, p = 1, d = 0, q = 4,details=FALSE)
model_5 = sarima(v, p = 1, d = 0, q = 5,details=FALSE)
model_6 = sarima(v, p = 1, d = 0, q = 6,details=FALSE)
```

With consideration of cross validation error, model AIC, model AICc, model BIC, the MA(4) model is the best one to approximate the second order differencing series v. For MA(4) model, the p values for Ljung-Box statistic are very large for all lag h, so MA(4) model passes the test. Besides MA(4) model, the MA(5) and ARMA(1,4) model serve as the second candidate group. In the perspective of cross validation error, ARMA(1,4) performs better than MA(5), while the thing is opposite for the aspect of AIC, AICc, and BIC  statistics. Since discrepancy of the three model's performance statistics is not very large, all the three models can be explored in the further parameter tuning experiment.
```{r}
model = c("MA(4)", "MA(5)","MA(6)","ARMA(1,4)","ARMA(1,5)","ARMA(1,6)")
cves = c(cv1,cv2,cv3,cv4,cv5,cv6)
aic = c(model_1$AIC,model_2$AIC,model_3$AIC,model_4$AIC,model_5$AIC,model_6$AIC)
aicc = c(model_1$AICc,model_2$AICc,model_3$AICc,model_4$AICc,model_5$AICc,model_6$AICc)
bic = c(model_1$BIC,model_2$BIC,model_3$BIC,model_4$BIC,model_5$BIC,model_6$BIC)
s <- data.frame("Model" = model, "Crossvalidation error"=cves,"AIC"=aic,"AICc"=aicc,"BIC"=bic)
s
```
```{r, echo=FALSE}
model_1 = sarima(v, p = 0, d = 0, q = 4)
model_2 = sarima(v, p = 0, d = 0, q = 5)
model_4 = sarima(v, p = 1, d = 0, q = 4)
```




# Model Comparision and Selection

# Final Model

## Model interpretation

## Prediction

# Conclusion