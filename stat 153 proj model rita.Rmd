---
title: "153-proj-20211116"
author: "Rita Zhang"
date: "11/16/2021"
output:
  pdf_document: default
  html_document: default
---
\newpage
```{r include = FALSE}
# Template source: https://github.com/alexpghayes/rmarkdown_homework_template
knitr::opts_chunk$set(
  echo = FALSE, # don't show code
  warning = FALSE, # don't show warnings
  message = FALSE, # don't show messages (less serious warnings)
  cache = FALSE, # set to TRUE to save results from last compilation
  fig.align = "center", # center figures
  fig.height = 4,
  out.width = "75%"
)
library(ggplot2)
library(patchwork)
library(astsa)
library(TSA)
library(forecast)
set.seed(153) # make random results reproducible
```

# Abstract

# Introduction

# Data Description

```{r}
# load data
stock = read.csv("../Desktop/TSLA.csv")
stock$Date = as.Date(stock$Date)
# Extract last 300 days
n = 300
t = 1:n
stock_300 = tail(stock, n)
stock_300 = stock_300[c('Date','Close')]
stock_300$t = t
```

# Exploratory Data Analysis

To obtain a comprehensive understanding of the data, we conduct explanatory data analysis (EDA) first. Figure 1(a) is the time series plot of all the given time points. We observe that the average price of Tesla before 2020 is considerably lower than that after 2020. Also, the stock price before 2020 seems to have a constant trend and no seasonality. Moreover, due to the excessive number of time points, it is difficult to visually examine the trend and seasonality pattern for data after 2020. Therefore, for the sake of interest and convenience, we decide to only analyze the last 300 time points, which cover the period from `2020-08-26` to `2021-11-02` excluding weekends. Thus, whenever we mention "data" in the following analysis, we implicitly mean the last 300 time points. 

```{r, fig.cap="(a) Time series plot of all available trading days. (b) Time series plot of last 300 trading days", fig.height=6}
# Line plot of all the data
full_data <- ggplot(data = stock, aes(x = Date, y = Close)) +
  geom_line() +
  ylab("Close Price (Dollars)") +
  ggtitle("(a)")
# Line plot of last 300 data
last_300 <- ggplot(data = stock_300, aes(x = Date, y = Close)) +
  geom_line() +
  ylab("Close Price (Dollars)") +
  ggtitle("(b)")
full_data / last_300
```
Figure 1(b) is the time series plot of the close prices of Tesla in last three hundred trading days before and including `2021-11-02`. We first observe that our data is roughly homoscedastic based on Figure 1(b), so we do not need to transform the data to stabilize the variance of the data.

```{r, fig.cap="(a) Time series plot of all available trading days. (b) Time series plot of last 300 trading days"}
# # Line plot of last 300 data with square root transformation
# last_300_sqrt <- ggplot(data = stock_300, aes(x = Date, y = Close)) +
#   geom_line() +
#   scale_y_sqrt() +
#   ggtitle("(a)")
# 
# # Line plot of last 300 data with natural log transformation
# last_300_log <- ggplot(data = stock_300, aes(x = Date, y = Close)) +
#   geom_line() +
#   scale_y_log10() +
#   ggtitle("(b)")
# 
# 
# last_300
# last_300_sqrt
# last_300_log
# ```
# ```{r, fig.cap="(a) Time series plot of all available trading days. (b) Time series plot of last 300 trading days"}
# par(mfrow = c(1, 2))
# 
# # Line plot of last 300 data with square root transformation
# stock_300$sqrt_close = sqrt(stock_300$Close)
# last_300_sqrt <- plot(t, stock_300$sqrt_close, type = 'l', xlab = 'Day')
# 
# # Line plot of last 300 data with natural log transformation
# stock_300$log_close = log(stock_300$Close)
# last_300_log <- plot(t, stock_300$log_close, type = 'l', xlab = 'Day')
```

Intuitively, the data is not stationary since there exists a nonlinear trend. We do not observe a clear seasonality pattern probably because the data is daily. To be more convincing, we plot the sample ACF and PACF of the data in Figure 2.

```{r, fig.cap="Sample ACF and PACF plots of the data", results='hide'}
acf2(stock_300$Close, max.lag = 50, main = "Close Price of Tesla")
```

The decaying, positive ACF's imply a possible trend. As we expect, both sample ACF and PACF plots do not demonstrate a seasonality pattern. To quantitatively verify our superposition that the data does not have a strong seasonality pattern, we plot and inspect the periodogram of the data.

```{r, fig.cap="The periodogram of the data"}
par(mfrow = c(1, 1), cex = 0.8)
periodogram(stock_300$Close)
```
Since the periodogram has multiple consecutive spikes, the phenomenon called "leakage", the data does not have a dominant seasonality pattern or frequency in our data. Nonetheless, we suspect a possible seasonality with period $d=5$ since five trading days can be viewed as a week in the stock market.

# Model Construction

With a comprehensive understanding of our data, we start to experiment and construct different time series model. We choose and build two non-parametric signal models of the trend and seasonality in our data. We aim to make the residuals approximately weekly stationary. We do not consider any parametric trend model because we think the trend of the stock price data is too complicated to be modeled by a parametric model, such as a high-order polynomial. Certainly, we could use a 15 or 20 order polynomial, but it may overfit the training data and produce imprecise predictions. We do not consider a parametric seasonality model either by the analysis at the end of the EDA section. Finally, based on each signal model, we provide two ARMA models or its extension, such as SARMA or ARIMA, to whiten the residuals of the signal model. Thus, we have four candidate models, and we will explain how we select a final model among them in the next section.

## Non-parametric Signal Model: exponential smoothing

In this model, we choose exponential smoothing with weight $\alpha = 0.8$ and lag $k = 10$ and a seasonal differencing with period $d = 5$. 

```{r, fig.cap="(a): Time series plot of the original data and fitted values. (b): The residual plot of exponential smoothing.", out.width="100%"}
par(mfrow = c(1, 2), cex = 0.65, lwd = 0.8)
# exponential filter 
alpha = 0.8
lag = 10
filter_weights = alpha^(1:lag)
filter_weights = filter_weights/sum(filter_weights)
filtered_stock = filter(stock_300$Close, filter_weights, sides = 1)
# Plot the original data and fitted values
plot(stock_300$t, stock_300$Close, type = 'l', main = "(a)", xlab = "Time", ylab = "Close Price")
lines(stock_300$t, filtered_stock, col = 'red')
filtered_stock = na.omit(filtered_stock)
log_stock = stock_300$Close[-1:-(lag - 1)]
res = log_stock - filtered_stock
plot(res, main = "(b)", xlab = "Time", ylab = "Residuals")
```

We experiment with different combinations of $\alpha$ and $k$ with a careful consideration of overfitting issue. we choose $k=10$ as the final value because we want to only use past two weeks, which are ten days in our data, to forecast. We choose $\alpha = 0.8$ as the final value because it best balances the smoothing effect and the capture of trend pattern. Indeed, the smoothing line in Figure 4(a) fits the data in the way that we want. Note that we lose the first nine time points due to the computation process of the smoothing filter.

However, the residual plot Figure 4(b) is fairly non-stationary, as it has cycling fluctuation pattern. We use the seasonal differencing with period $d = 5$, which is one week in our data, on the residuals to remove the pattern.

```{r, results='hide', fig.cap="(a): Time series plot of the seasonal differenced (d = 5) residuals from the previous smoothing."}
# seasonal differencing
diff_res = diff(res, lag = 5)
plot(diff_res, main = "(a)", ylab = "Differenced Data")
```

We believe that the time series of the differenced residuals shown in Figure 5(a) is sufficiently stationary.

## Non-parametric Model: second order differencing 

In this model, we choose a second-order differencing. We observe that after the first-order differencing, there is still some trend pattern, such as the increasing one between $270$ and $300$, as shown by Figure TODO. Thus, we take another differencing and acquire the second-order differencing data shown in Figure TODO.

```{r, fig.cap="(a): The first-order differenced data. (b): The second-order differenced data.", out.width="100%"}
par(mfrow = c(1, 2), cex = 0.65, lwd = 0.5)
# first order differencing
stock_d = diff(stock_300$Close)
# second order differencing
stock_dd = diff(stock_d)
plot(stock_d, type = 'l', main = "(a)", ylab = "Differenced Data")
plot(stock_dd, type = 'l', main = "(b)", ylab = "Differenced Data")
```
The second-order differenced data looks more stationary than the first-order differenced data. Moreover, we are satisfied with the approximate stationarity of the second-order data, so we do not need to consider the seasonality any further.


```{r, fig.cap="(a): Second-order and seasonal differenced (d = 5) data"}
# # second order differencing plus seasonal differencing
# stock_dd_5d = diff(stock_dd, lag = 5)
# 
# plot(stock_dd_5d, type = 'l', main = "(a)", ylab = "Differenced Data")
```

# Model Comparision and Selection

## Model 2 (Ruojia Zhang)

```{r}
model2 <- sarima(diff_res, p = 1, d = 0, q = 1, P = 0, D = 1, Q = 2, S = 5)
```

Comments:

From the acf plot of the residuals, we can see that most of the sample residuals autocorrelations fit nicely inside the confidence band. From the normal q-q plot of the stanard residuals we can see that normal distribution is a convincing assumption. We also have large p-values that suggests weak groupwise residual autocorrelations in the plot for Ljung-Box statistic. The model seems like a good fit.

Then we report the AIC, AICc, BIC values for the model:

```{r}
model2$AIC
```

```{r}
model2$AICc
```

```{r}
model2$BIC
```

We perform time-series cross validation on the last 100 data points. For the model, we generate forecasts for the 10 days in the selected 2 weeks and compute the sum of squares of errors of the forecasts. Then average the sum of squares of errors of forecasts over the days considered, which generates the mse of this model.

```{r}
# perform cross validation evaluation
start_day <- 250
end_day <- 290
jump = 10
error = 0
forward_time = 10
data = tail(stock$Close , 300)
# we are performing n-fold validation 
n = (end_day - start_day) / jump
for (k in start_day:end_day) {
  train <- window(data, end=k-0.001)
  test <- window(data, start=k, end=k + forward_time - 0.01)
  res_model <- sarima.for(diff_res, n.ahead = forward_time, p = 1, d = 0, q = 1, P = 0, D = 1, Q = 2, S = 5)
  trend_prediction <- ses(train, alpha=0.9, h=forward_time)
  res_prediction <- res_model$pred
  print(trend_prediction)
  error <- error + sum((res_prediction - test)**2)
  k = k + 10
}
```

```{r}
cv_model2 = error / n
cat("cross validation error", cv_model2)
```


# Final Model

## Model interpretation

## Prediction

# Conclusion